{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n"
      ],
      "metadata": {
        "id": "qHfkshedSRyH"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython import get_ipython\n",
        "from IPython.display import display"
      ],
      "metadata": {
        "id": "bvDnqyfSMn70"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuSSeoFwlz-Y",
        "outputId": "8211ae19-b9e0-487c-a344-ae2f05d85c8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'fewshot-face-translation-GAN' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "# Clone dev branch for the latest model\n",
        "!git clone -b dev --recursive https://github.com/shaoanlu/fewshot-face-translation-GAN.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "bfGNntU5LfeX"
      },
      "outputs": [],
      "source": [
        "# There are import errors under keras == 2.2.5\n",
        "!pip install --upgrade tensorflow keras --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd fewshot-face-translation-GAN"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kizuaZ1nOXBn",
        "outputId": "473b467f-ccd2-47de-f17c-0d80a0b69815"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/fewshot-face-translation-GAN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUDjTRuiLfeZ",
        "outputId": "f2244f1d-10ef-404d-f2ba-0fc127f39576"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ThyUhXkVY5WJIRbik_Jpx9vOqEQ4jEZw\n",
            "To: /content/fewshot-face-translation-GAN/encoder.h5\n",
            "100% 6.26M/6.26M [00:00<00:00, 87.3MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1cVkPjTxqQXo0pyLQHw_Yhngc4GO6KB5Q\n",
            "From (redirected): https://drive.google.com/uc?id=1cVkPjTxqQXo0pyLQHw_Yhngc4GO6KB5Q&confirm=t&uuid=fc24bf8a-5830-495c-b800-23e0dd01c3f7\n",
            "To: /content/fewshot-face-translation-GAN/decoder.h5\n",
            "100% 126M/126M [00:01<00:00, 65.3MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Download pre-trined weights\n",
        "!gdown https://drive.google.com/uc?id=1ThyUhXkVY5WJIRbik_Jpx9vOqEQ4jEZw\n",
        "!gdown https://drive.google.com/uc?id=1cVkPjTxqQXo0pyLQHw_Yhngc4GO6KB5Q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9b2N8INnm8P",
        "outputId": "39c4b46f-d3ec-4113-a105-a6d1867c9b80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘weights’: File exists\n"
          ]
        }
      ],
      "source": [
        "!mkdir weights\n",
        "!mv decoder.h5 weights/decoder.h5\n",
        "!mv encoder.h5 weights/encoder.h5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QcdF2i9umQGk",
        "outputId": "0c22d910-14cd-4340-cd32-16c6a6b01be4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "colab_demo.ipynb    fewshot-face-translation-GAN  preprocess.py  utils\n",
            "configs\t\t    images\t\t\t  __pycache__\t weights\n",
            "data\t\t    models.py\t\t\t  README.md\n",
            "face_toolbox_keras  networks\t\t\t  train.py\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dc7-w7eLfeb"
      },
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tj4uhc8dLfeb",
        "outputId": "3af0727c-f3e3-4b60-9830-3af4335abe00"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dir_weights': './weights',\n",
              " 'identity_extractor': 'inceptionresnetv1',\n",
              " 'input_size': 224,\n",
              " 'latent_dim': 512,\n",
              " 'separate_adain': True,\n",
              " 'additional_emb': True,\n",
              " 'use_nwg': False,\n",
              " 'nc_in': 3}"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "from utils.config_loader import load_yaml\n",
        "config = load_yaml(\"configs/config_inference.yaml\"); config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htzgMCcYlz_G"
      },
      "source": [
        "## Load GAN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "PDCd3QM_lz_J"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Navigate to the models.py file in the cloned repository and modify it.\n",
        "# Replace the line importing Beta from tensorflow.contrib\n",
        "!sed -i \"s/from tensorflow.contrib.distributions import Beta/from tensorflow.compat.v1.distributions import Beta/\" /content/fewshot-face-translation-GAN/fewshot-face-translation-GAN/models.py"
      ],
      "metadata": {
        "id": "1_0KRNr0M9js"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/fewshot-face-translation-GAN/models.py\"\n",
        "\n",
        "with open(file_path, \"r\") as file:\n",
        "    code = file.read()\n",
        "\n",
        "# Remove old contrib import\n",
        "code = code.replace(\"from tensorflow.contrib.distributions import Beta\",\n",
        "                    \"import tensorflow_probability as tfp\\nBeta = tfp.distributions.Beta\")\n",
        "\n",
        "with open(file_path, \"w\") as file:\n",
        "    file.write(code)\n",
        "\n",
        "print(\"Updated import for Beta distribution using TensorFlow Probability.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1L_Lt6iObuOT",
        "outputId": "e6638e79-2ecf-43b4-bf7f-64a31212d047"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated import for Beta distribution using TensorFlow Probability.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/fewshot-face-translation-GAN/networks/nn_blocks.py\"\n",
        "\n",
        "with open(file_path, \"r\") as f:\n",
        "    code = f.read()\n",
        "\n",
        "# Fix invalid import\n",
        "code = code.replace(\n",
        "    \"from keras.layers.advanced_activations import LeakyReLU\",\n",
        "    \"from keras.layers import LeakyReLU\"\n",
        ")\n",
        "\n",
        "with open(file_path, \"w\") as f:\n",
        "    f.write(code)\n",
        "\n",
        "print(\"Fixed LeakyReLU import in nn_blocks.py\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OFdAeJSb-f9",
        "outputId": "9cbd0cc6-f35f-4788-9a9e-ed99e6703654"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fixed LeakyReLU import in nn_blocks.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/fewshot-face-translation-GAN/networks/instance_normalization.py\"\n",
        "\n",
        "with open(file_path, \"r\") as f:\n",
        "    code = f.read()\n",
        "\n",
        "# Fix Layer + InputSpec import\n",
        "code = code.replace(\n",
        "    \"from keras.engine import Layer, InputSpec\",\n",
        "    \"from tensorflow.keras.layers import Layer, InputSpec\"\n",
        ")\n",
        "\n",
        "# Ensure all other keras.* are tensorflow.keras.*\n",
        "code = code.replace(\"from keras.\", \"from tensorflow.keras.\")\n",
        "code = code.replace(\"import keras.\", \"import tensorflow.keras.\")\n",
        "\n",
        "with open(file_path, \"w\") as f:\n",
        "    f.write(code)\n",
        "\n",
        "print(\"Fixed imports in instance_normalization.py\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fesDNesBcJkn",
        "outputId": "b79f4057-4cb5-4115-be88-30cb390ea553"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fixed imports in instance_normalization.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find /content/fewshot-face-translation-GAN -type f -name \"*.py\" -exec sed -i 's/tf\\.get_default_graph()/tf.compat.v1.get_default_graph()/g' {} \\;\n",
        "\n",
        "print(\"Replaced tf.get_default_graph() with tf.compat.v1.get_default_graph()\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwO5oblYQE_J",
        "outputId": "fd5621fc-1ed2-4391-841a-34463577570c"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Replaced tf.get_default_graph() with tf.compat.v1.get_default_graph()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/fewshot-face-translation-GAN/networks/instance_normalization.py\"\n",
        "\n",
        "with open(file_path, \"r\") as f:\n",
        "    code = f.read()\n",
        "\n",
        "# Fix Layer + InputSpec import\n",
        "code = code.replace(\n",
        "    \"from keras.engine import Layer, InputSpec\",\n",
        "    \"from tensorflow.keras.layers import Layer, InputSpec\"\n",
        ")\n",
        "\n",
        "# Fix the import path for get_custom_objects\n",
        "code = code.replace(\n",
        "    \"from tensorflow.keras.utils.generic_utils import get_custom_objects\",\n",
        "    \"from tensorflow.keras.utils import get_custom_objects\"\n",
        ")\n",
        "\n",
        "\n",
        "# Ensure all other keras.* are tensorflow.keras.*\n",
        "code = code.replace(\"from keras.\", \"from tensorflow.keras.\")\n",
        "code = code.replace(\"import keras.\", \"import tensorflow.keras.\")\n",
        "\n",
        "with open(file_path, \"w\") as f:\n",
        "    f.write(code)\n",
        "\n",
        "print(\"Fixed imports in instance_normalization.py\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOx9n_v0c5z6",
        "outputId": "75b023a5-f48a-4859-89ca-d682d3bb3042"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fixed imports in instance_normalization.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/fewshot-face-translation-GAN/networks/discriminator.py\"\n",
        "\n",
        "with open(file_path, \"r\") as f:\n",
        "    code = f.read()\n",
        "\n",
        "# Fix invalid import for LeakyReLU\n",
        "code = code.replace(\n",
        "    \"from keras.layers.advanced_activations import LeakyReLU\",\n",
        "    \"from keras.layers import LeakyReLU\"\n",
        ")\n",
        "\n",
        "with open(file_path, \"w\") as f:\n",
        "    f.write(code)\n",
        "\n",
        "print(\"Fixed LeakyReLU import in discriminator.py\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdBfTyLsdDZO",
        "outputId": "ae426442-fb10-44f3-e5fd-ee6fa6f6ca75"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fixed LeakyReLU import in discriminator.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/fewshot-face-translation-GAN/networks/instance_normalization.py\"\n",
        "\n",
        "with open(file_path, \"r\") as f:\n",
        "    code = f.read()\n",
        "\n",
        "# Ensure tensorflow is imported as tf\n",
        "if \"import tensorflow as tf\" not in code:\n",
        "    code = \"import tensorflow as tf\\n\" + code\n",
        "\n",
        "# Fix Layer + InputSpec import\n",
        "code = code.replace(\n",
        "    \"from keras.engine import Layer, InputSpec\",\n",
        "    \"from tensorflow.keras.layers import Layer, InputSpec\"\n",
        ")\n",
        "\n",
        "# Fix the import path for get_custom_objects if it exists and is incorrect\n",
        "code = code.replace(\n",
        "    \"from tensorflow.keras.utils.generic_utils import get_custom_objects\",\n",
        "    \"from tensorflow.keras.utils import get_custom_objects\"\n",
        ")\n",
        "\n",
        "# Ensure all other keras.* are tensorflow.keras.* imports\n",
        "# This is a broad replacement, be careful if it causes other issues,\n",
        "# but it's necessary for the migration.\n",
        "code = code.replace(\"from keras.\", \"from tensorflow.keras.\")\n",
        "code = code.replace(\"import keras.\", \"import tensorflow.keras.\")\n",
        "\n",
        "# Find the call method and replace the int_shape line\n",
        "# This is a more targeted fix for the specific error location\n",
        "# Look for lines that look like 'input_shape = ...int_shape(inputs)'\n",
        "import re\n",
        "# Use regex to find lines getting input_shape using int_shape\n",
        "# This pattern is more robust to whitespace variations\n",
        "code = re.sub(r'input_shape\\s*=\\s*.*?int_shape\\(inputs\\)',\n",
        "              'input_shape = tf.shape(inputs)',\n",
        "              code)\n",
        "\n",
        "# Remove any leftover keras.backend imports if they exist and aren't used elsewhere\n",
        "code = re.sub(r'import keras\\.backend\\s*as\\s*K', '', code)\n",
        "code = re.sub(r'from keras\\s*import\\s*backend\\s*as\\s*K', '', code)\n",
        "\n",
        "\n",
        "with open(file_path, \"w\") as f:\n",
        "    f.write(code)\n",
        "\n",
        "print(\"Fixed int_shape usage in InstanceNormalization.call method using tf.shape.\")"
      ],
      "metadata": {
        "id": "kZ5EmcDQml0-",
        "outputId": "50cfe228-bf52-4baa-b71c-8f911b86b253",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fixed int_shape usage in InstanceNormalization.call method using tf.shape.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/fewshot-face-translation-GAN/networks/instance_normalization.py\"\n",
        "\n",
        "with open(file_path, \"r\") as f:\n",
        "    code = f.read()\n",
        "\n",
        "# Ensure tensorflow is imported as tf\n",
        "if \"import tensorflow as tf\" not in code:\n",
        "    code = \"import tensorflow as tf\\n\" + code\n",
        "\n",
        "# Fix Layer + InputSpec import\n",
        "code = code.replace(\n",
        "    \"from keras.engine import Layer, InputSpec\",\n",
        "    \"from tensorflow.keras.layers import Layer, InputSpec\"\n",
        ")\n",
        "\n",
        "# Fix the import path for get_custom_objects if it exists and is incorrect\n",
        "code = code.replace(\n",
        "    \"from tensorflow.keras.utils.generic_utils import get_custom_objects\",\n",
        "    \"from tensorflow.keras.utils import get_custom_objects\"\n",
        ")\n",
        "\n",
        "# Ensure all other keras.* are tensorflow.keras.* imports\n",
        "# This is a broad replacement, be careful if it causes other issues,\n",
        "# but it's necessary for the migration.\n",
        "code = code.replace(\"from keras.\", \"from tensorflow.keras.\")\n",
        "code = code.replace(\"import keras.\", \"import tensorflow.keras.\")\n",
        "\n",
        "# Explicitly replace keras.backend.int_shape with tf.shape within the code,\n",
        "# especially targeting the call method if possible, but a general replacement\n",
        "# should also work if keras.backend is only used for int_shape.\n",
        "# Let's specifically target the int_shape usage.\n",
        "code = code.replace(\"keras.backend.int_shape(inputs)\", \"tf.shape(inputs)\")\n",
        "# Also handle if it's just K.int_shape(inputs) after an import 'import keras.backend as K'\n",
        "code = code.replace(\"K.int_shape(inputs)\", \"tf.shape(inputs)\")\n",
        "\n",
        "\n",
        "# Remove any leftover keras.backend imports if they exist and aren't used elsewhere\n",
        "code = re.sub(r'import keras\\.backend\\s*as\\s*K', '', code)\n",
        "code = re.sub(r'from keras\\s*import\\s*backend\\s*as\\s*K', '', code)\n",
        "\n",
        "\n",
        "with open(file_path, \"w\") as f:\n",
        "    f.write(code)\n",
        "\n",
        "print(\"Fixed int_shape usage in InstanceNormalization.call method using tf.shape and removed backend imports.\")"
      ],
      "metadata": {
        "id": "n2FG3e7Poe4d",
        "outputId": "5023eb11-a18b-47d3-def7-96ec081401b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fixed int_shape usage in InstanceNormalization.call method using tf.shape and removed backend imports.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import re\n",
        "\n",
        "file_path_norm = \"/content/fewshot-face-translation-GAN/networks/instance_normalization.py\"\n",
        "file_path_models = \"/content/fewshot-face-translation-GAN/models.py\"\n",
        "\n",
        "# --- Fix Instance Normalization ---\n",
        "try:\n",
        "    with open(file_path_norm, \"r\") as f:\n",
        "        code = f.read()\n",
        "\n",
        "    # Ensure tensorflow is imported as tf\n",
        "    if \"import tensorflow as tf\" not in code:\n",
        "        code = \"import tensorflow as tf\\n\" + code\n",
        "\n",
        "    # Fix Layer + InputSpec import\n",
        "    code = code.replace(\n",
        "        \"from keras.engine import Layer, InputSpec\",\n",
        "        \"from tensorflow.keras.layers import Layer, InputSpec\"\n",
        "    )\n",
        "\n",
        "    # Fix the import path for get_custom_objects if it exists and is incorrect\n",
        "    code = code.replace(\n",
        "        \"from tensorflow.keras.utils.generic_utils import get_custom_objects\",\n",
        "        \"from tensorflow.keras.utils import get_custom_objects\"\n",
        "    )\n",
        "\n",
        "    # Ensure all other keras.* are tensorflow.keras.* imports\n",
        "    code = code.replace(\"from keras.\", \"from tensorflow.keras.\")\n",
        "    code = code.replace(\"import keras.\", \"import tensorflow.keras.\")\n",
        "\n",
        "    # Replace keras.backend.int_shape or K.int_shape with tf.shape\n",
        "    code = code.replace(\"keras.backend.int_shape(inputs)\", \"tf.shape(inputs)\")\n",
        "    code = code.replace(\"K.int_shape(inputs)\", \"tf.shape(inputs)\")\n",
        "\n",
        "    # --- FIX: Replace the core calculation block in the call method ---\n",
        "    # We need to find the 'call' method definition.\n",
        "    call_method_pattern = r\"(def\\s+call\\(self,\\s+inputs,\\s*training\\s*=\\s*None\\):.*?)(\\n\\s*def\\s+|$)\" # Capture the call method block\n",
        "\n",
        "    call_method_match = re.search(call_method_pattern, code, re.DOTALL) # Use DOTALL to match across lines\n",
        "\n",
        "    if call_method_match:\n",
        "        call_method_block = call_method_match.group(1)\n",
        "\n",
        "        # Define the correct calculation block with proper indentation\n",
        "        # We'll try to find the indentation from the 'def call' line itself\n",
        "        indent_match = re.match(r\"(\\s*)def\\s+call\", call_method_block)\n",
        "        indent = indent_match.group(1) if indent_match else \"\" # Get leading whitespace\n",
        "\n",
        "        # We'll replace the *entire* content of the call method with the correct version\n",
        "        # to be sure no old code remains. This is safer than trying to surgically\n",
        "        # replace parts of the block.\n",
        "        new_call_method_content = f\"\"\"\n",
        "{indent}    # Get input shape and rank symbolically\n",
        "{indent}    input_shape = tf.shape(inputs)\n",
        "{indent}    input_rank = tf.rank(inputs)\n",
        "{indent}\n",
        "{indent}    # Instance normalization axes exclude batch (0) and channels (last)\n",
        "{indent}    # reduction_axes should cover spatial dimensions (1 to rank-2)\n",
        "{indent}    # Ensure rank is at least 3 (batch, spatial, channel) for instance norm\n",
        "{indent}    tf.debugging.assert_greater_equal(input_rank, 3, message=\"Input rank must be at least 3 for instance normalization.\")\n",
        "{indent}    # Calculate reduction axes dynamically based on rank\n",
        "{indent}    reduction_axes = tf.range(1, input_rank - 1) # Exclude batch (0) and channel (rank-1)\n",
        "{indent}\n",
        "{indent}    # Calculate mean and variance over the reduction axes\n",
        "{indent}    # Use tf.nn.moments for numerical stability and symbolic compatibility\n",
        "{indent}    mean, variance = tf.nn.moments(inputs, axes=reduction_axes, keepdims=True)\n",
        "{indent}\n",
        "{indent}    # Add epsilon for numerical stability\n",
        "{indent}    variance = variance + self.epsilon\n",
        "{indent}\n",
        "{indent}    # Normalization formula: (x - mean) / sqrt(variance + epsilon) * gamma + beta\n",
        "{indent}    inv = tf.math.rsqrt(variance)\n",
        "{indent}    normalized = (inputs - mean) * inv\n",
        "{indent}\n",
        "{indent}    # Apply scale (gamma) and offset (beta) if they exist\n",
        "{indent}    if self.scale:\n",
        "{indent}        normalized = normalized * self.gamma\n",
        "{indent}    if self.center:\n",
        "{indent}        normalized = normalized + self.beta\n",
        "{indent}\n",
        "{indent}    return normalized\n",
        "\"\"\"\n",
        "        # Replace the old call method block with the new one.\n",
        "        # Find the exact span of the original call method block (including the def line)\n",
        "        call_method_span = call_method_match.span()\n",
        "        code = code[:call_method_span[0]] + new_call_method_content + code[call_method_span[1]:]\n",
        "\n",
        "        print(\"Replaced the entire InstanceNormalization.call method with a standard implementation.\")\n",
        "\n",
        "        # Final check: search for 'len(' within the modified code for the call method\n",
        "        call_method_match_after = re.search(call_method_pattern, code, re.DOTALL)\n",
        "        if call_method_match_after:\n",
        "             call_method_block_after = call_method_match_after.group(1)\n",
        "             if 'len(' in call_method_block_after:\n",
        "                 print(\"Warning: Found 'len(' still present in the modified InstanceNormalization.call method.\")\n",
        "             else:\n",
        "                 print(\"Checked: 'len(' not found in the modified InstanceNormalization.call method.\")\n",
        "\n",
        "\n",
        "    else:\n",
        "         print(\"Could not find the 'call' method definition in InstanceNormalization.py\")\n",
        "\n",
        "\n",
        "    # Remove any leftover keras.backend imports if they exist and aren't used elsewhere\n",
        "    code = re.sub(r'import keras\\.backend\\s*as\\s*K', '', code)\n",
        "    code = re.sub(r'from keras\\s*import\\s+backend\\s*as\\s*K', '', code)\n",
        "\n",
        "    with open(file_path_norm, \"w\") as f:\n",
        "        f.write(code)\n",
        "\n",
        "    print(\"Attempted comprehensive fix for InstanceNormalization.call method logic.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found at {file_path_norm}\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred while fixing InstanceNormalization: {e}\")\n",
        "\n",
        "\n",
        "# --- Remove generic exception in models.py ---\n",
        "try:\n",
        "    with open(file_path_models, \"r\") as f:\n",
        "        code_models = f.read()\n",
        "\n",
        "    # Find the __init__ method block\n",
        "    init_method_pattern = r\"(def\\s+__init__\\(self,\\s+config\\):.*?)(\\n\\s*def\\s+|$)\"\n",
        "    init_method_match = re.search(init_method_pattern, code_models, re.DOTALL)\n",
        "\n",
        "    if init_method_match:\n",
        "        init_method_block = init_method_match.group(1)\n",
        "\n",
        "        # Replace the try...except block with just the content inside the try\n",
        "        # assuming the content inside is the correct initialization logic\n",
        "        # This pattern looks for the try...except around build_encoder and build_decoder\n",
        "        try_except_pattern = r\"(\\s*)try:(\\s*self\\.encoder\\s*=\\s*self\\.build_encoder\\(\\)\\s*self\\.decoder\\s*=\\s*self\\.build_decoder\\(\\))(\\s*)except:(\\s*raise Exception\\(\\\"Error building networks\\.\\\"\\))\"\n",
        "        updated_init_block = re.sub(try_except_pattern, r\"\\1\\2\", init_method_block, flags=re.DOTALL)\n",
        "\n",
        "        if updated_init_block != init_method_block:\n",
        "             # Replace the old init block with the updated one\n",
        "            init_method_span = init_method_match.span()\n",
        "            code_models = code_models[:init_method_span[0]] + updated_init_block + code_models[init_method_span[1]:]\n",
        "            print(\"Removed generic try...except block in models.py __init__.\")\n",
        "        else:\n",
        "             print(\"Could not find the generic try...except block in models.py __init__ to remove.\")\n",
        "\n",
        "    else:\n",
        "        print(\"Could not find the __init__ method definition in models.py\")\n",
        "\n",
        "\n",
        "    with open(file_path_models, \"w\") as f:\n",
        "        f.write(code_models)\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found at {file_path_models}\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred while fixing models.py: {e}\")"
      ],
      "metadata": {
        "id": "PPCPA4CrtDSu",
        "outputId": "a45d3308-443a-4d8a-cb87-bb8c2fab41a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Replaced the entire InstanceNormalization.call method with a standard implementation.\n",
            "Attempted comprehensive fix for InstanceNormalization.call method logic.\n",
            "Could not find the generic try...except block in models.py __init__ to remove.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "SByaoidilz_O",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from models import FaceTranslationGANInferenceModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 670
        },
        "id": "LRheogi-lz_U",
        "outputId": "4c32b468-4909-4b6e-ef2c-634383a2e78f",
        "scrolled": false
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "Error building networks.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/content/fewshot-face-translation-GAN/models.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_decoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/fewshot-face-translation-GAN/models.py\u001b[0m in \u001b[0;36mbuild_encoder\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbuild_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnc_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/fewshot-face-translation-GAN/networks/generator.py\u001b[0m in \u001b[0;36mencoder\u001b[0;34m(nc_in, input_size)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp_ref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp_segm_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mconv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mconv2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/fewshot-face-translation-GAN/networks/nn_blocks.py\u001b[0m in \u001b[0;36mconv_block\u001b[0;34m(input_tensor, f, use_norm, k, strides)\u001b[0m\n\u001b[1;32m     45\u001b[0m                kernel_initializer=conv_init, use_bias=(not use_norm))(x)\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0muse_norm\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mActivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/fewshot-face-translation-GAN/networks/nn_blocks.py\u001b[0m in \u001b[0;36mnormalization\u001b[0;34m(inp, norm, group)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mnorm\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'instancenorm'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInstanceNormalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0;31m#elif norm == \"SPADE_norm\":\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/fewshot-face-translation-GAN/networks/instance_normalization.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;31m# Get input shape and rank symbolically\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m     \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m     \u001b[0minput_rank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Exception encountered when calling InstanceNormalization.call().\n\n\u001b[1mCould not automatically infer the output shape / dtype of 'instance_normalization_11' (of type InstanceNormalization). Either the `InstanceNormalization.call()` method is incorrect, or you need to implement the `InstanceNormalization.compute_output_spec() / compute_output_shape()` method. Error encountered:\n\nlen is not well defined for a symbolic Tensor (Shape:0). Please call `x.shape` rather than `len(x)` for shape information.\u001b[0m\n\nArguments received by InstanceNormalization.call():\n  • args=('<KerasTensor shape=(None, 112, 112, 64), dtype=float32, sparse=False, ragged=False, name=keras_tensor_95>',)\n  • kwargs=<class 'inspect._empty'>",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-79-212219274.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFaceTranslationGANInferenceModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/fewshot-face-translation-GAN/models.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_decoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error building networks.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdir_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dir_weights\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdir_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: Error building networks."
          ]
        }
      ],
      "source": [
        "model = FaceTranslationGANInferenceModel(config=config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpyAOeYXlz_w"
      },
      "outputs": [],
      "source": [
        "from face_toolbox_keras.models.verifier.face_verifier import FaceVerifier\n",
        "fv = FaceVerifier(classes=512)\n",
        "\n",
        "from face_toolbox_keras.models.parser import face_parser\n",
        "fp = face_parser.FaceParser()\n",
        "\n",
        "from face_toolbox_keras.models.detector import face_detector\n",
        "fd = face_detector.FaceAlignmentDetector()\n",
        "\n",
        "from face_toolbox_keras.models.detector.iris_detector import IrisDetector\n",
        "idet = IrisDetector()\n",
        "#idet.set_detector(fd)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_3cxE0-Lfee"
      },
      "source": [
        "## Upload test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PoFePLoelz__"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from utils import utils\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJy8TIc3m0n_"
      },
      "outputs": [],
      "source": [
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oduY1uoSl0AI"
      },
      "outputs": [],
      "source": [
        "# Upload a source face image.\n",
        "# There should be only one source face image.\n",
        "fn_src = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GAw05ir2l0AN"
      },
      "outputs": [],
      "source": [
        "# Upload target face images.\n",
        "# Number of target face images is not restricted as long as they belong to the same identity.\n",
        "fns_tar = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gvoYSeCoghg"
      },
      "outputs": [],
      "source": [
        "# Set input image path\n",
        "fn_src = [k for k,v in fn_src.items()]\n",
        "if len(fn_src) >= 1:\n",
        "    fn_src = fn_src[0]\n",
        "\n",
        "fns_tar = [k for k,v in fns_tar.items()]\n",
        "\n",
        "print(fn_src)\n",
        "print(fns_tar)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8tAH7KwLfef"
      },
      "outputs": [],
      "source": [
        "# We can also manually assign filenmes using the following code\n",
        "#fn_src = \"test02.jpg\"\n",
        "#fns_tar = [\"test01.jpg\", \"test03.jpg\", \"test04.jpg\", \"test05.jpg\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8rD90Ljlz_5"
      },
      "source": [
        "## Translate faces"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drpzzGCgl0AW"
      },
      "source": [
        "### Inferece\n",
        "\n",
        "It requires additional time to load models for the first infernce."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OGaNnRHkl0AX"
      },
      "outputs": [],
      "source": [
        "src, mask, aligned_im, (x0, y0, x1, y1), landmarks = utils.get_src_inputs(fn_src, fd, fp, idet, identity_extractor=config[\"identity_extractor\"])\n",
        "tar, emb_tar = utils.get_tar_inputs(fns_tar, fd, fv, identity_extractor=config[\"identity_extractor\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qeleIZzwl0Ag"
      },
      "outputs": [],
      "source": [
        "out = model.inference(src, mask, tar, emb_tar)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqTEOcqxl0Ak"
      },
      "source": [
        "### Visualize results\n",
        "\n",
        "Images are resized to having maximum side length of 768."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HqdLaD6Lfeg"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(src)\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(tar)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0P-d5H1Lfeg"
      },
      "outputs": [],
      "source": [
        "result_face = np.squeeze(((out[0] + 1) * 255 / 2).astype(np.uint8))\n",
        "plt.imshow(result_face)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44qXGZLkLfeg"
      },
      "outputs": [],
      "source": [
        "result_img = utils.post_process_result(fn_src, fd, result_face, aligned_im, src, x0, y0, x1, y1, landmarks)\n",
        "plt.imshow(result_img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Do9x9wZl0BK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "test.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}